---

copyright:
  years: 2014, 2025
lastupdated: "2025-12-05"


keywords: oks, iro, openshift, red hat, red hat openshift

subcollection: openshift

---


{{site.data.keyword.attribute-definition-list}}





# Logging for clusters
{: #health}

For cluster and app logs, {{site.data.keyword.openshiftlong}} clusters include built-in tools to help you manage the health of your single cluster instance. You can also set up {{site.data.keyword.cloud_notm}} tools for multi-cluster analysis or other use cases, such as {{site.data.keyword.containerlong_notm}} cluster add-ons: {{site.data.keyword.logs_full_notm}} and {{site.data.keyword.mon_full_notm}}.
{: shortdesc}

## Understanding options for logging
{: #oc_logmet_options}

To help understand when to use the built-in {{site.data.keyword.redhat_openshift_notm}} tools or {{site.data.keyword.cloud_notm}} integrations, review the following information.
{: shortdesc}

{{site.data.keyword.logs_full_notm}}
:   Customizable user interface for live streaming of log tailing, real-time troubleshooting issue alerts, and log archiving.
    - Quick integration with the cluster via a script.
    - Aggregated logs across clusters and cloud providers.
    - Historical access to logs that is based on the plan you choose.
    - Highly available, scalable, and compliant with industry security standards.
    - Integrated with {{site.data.keyword.cloud_notm}} IAM for user access management.
    
:   View cluster management events that are generated by the {{site.data.keyword.openshiftlong_notm}} API. To access these logs, [provision an instance of {{site.data.keyword.logs_full_notm}}](/docs/cloud-logs?topic=cloud-logs-getting-started). For more information about the types of {{site.data.keyword.containerlong_notm}} events that you can track, see [Activity Tracker events](/docs/openshift?topic=openshift-at_events_ref).

Built-in {{site.data.keyword.redhat_openshift_notm}} logging tools
:   Built-in view of pod logs in the {{site.data.keyword.redhat_openshift_notm}} web console.
    - Built-in pod logs are not configured with persistent storage. You must integrate with a cloud database to back up the logging data and make it highly available, and manage the logs yourself.
  
    To set up an [OpenShift Container Platform Elasticsearch, Fluentd, and Kibana EFK stack](https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/logging/index){: external}, see [installing the cluster logging operator](#oc_logging_operator). Keep in mind that your worker nodes must have at least 4 cores and GB memory to run the cluster logging stack.
    {: note}
  
Built-in {{site.data.keyword.redhat_openshift_notm}} audit logging tools
:   API audit logging to monitor user-initiated activities is currently not supported.


## Migrating logging and monitoring agents to Cloud Logs
{: #openshift_logging}

The observability CLI plug-in `ibmcloud ob` and the `v2/observe` endpoints are no longer supported. There is no direct replacement, but you can now manage your logging and monitoring integrations from the console or through the Helm charts. For the latest steps, [Deploying the Logging agent for OpenShift clusters](/docs/cloud-logs?topic=cloud-logs-agent-helm-os-deploy) and [Monitoring a Red Hat OpenShift cluster](/docs/monitoring?topic=monitoring-openshift_cluster).
{: unsupported}

You can no longer use the `ob` plug-in, Terraform, or API to install observability agents on a cluster or to modify your existing configuration. Sysdig agents continue to send metrics to the specified IBM Cloud Monitoring instance. LogDNA agents can no longer send logs since IBM Cloud Log Analysis is replaced by IBM Cloud Logs. 

### Reviewing your observability agents
{: #ob-review}

The observability plug-in installs Sysdig and LogDNA agents in the `ibm-observe` namespace.

1. [Access your {{site.data.keyword.redhat_openshift_notm}} cluster](/docs/openshift?topic=openshift-access_cluster).

2) Review the configmaps in the `ibm-observe` namespace.
    ```sh
    kubectl get cm -n ibm-observe
    ```
    {: pre}

    ```sh
    Example output

    NAME                                   DATA   AGE

    e405f1fc-feba-4350-9337-e7e249af871c   6      25m

    f59851a6-ede6-4719-afa0-eee7ce65eeb5   6      20m
    ```
    {: pre}

1. Observability agents installed by the observability plug-in use a configmap with the GUID of the IBM Cloud Monitoring instance or the IBM Cloud Log Analysis instance that logs or metrics are being sent to. If your cluster has agents in a namespace other than `ibm-observe` or the configmaps in `ibm-observe` are not named with the instance GUIDs, then these agents were not installed with the IKS observability (ob) plug-in.

### Removing the observability plug-in agents
{: #ob-remove}

1. Clean up the daemonsets and configmaps.
    ```sh
    kubectl delete daemonset logdna-agent -n ibm-observe
    kubectl delete daemonset sysdig-agent -n ibm-observe
    kubectl delete configmap <logdna-configmap> -n ibm-observe
    kubectl delete configmap <sysdig-configmap> -n ibm-observe
    ```
    {: pre}

1. Optional: Delete the namespace. After no other resources are running in the namespace.
    ```sh
    kubectl delete namespace ibm-observe
    ```
    {: pre}

After removing the plug-in has been removed, reinstall Logging and Monitoring agents in your cluster using the Cluster dashboard, Terraform, or manually. 

For more information, see the following links:
- [Deploying the Logging agent for OpenShift clusters](/docs/cloud-logs?topic=cloud-logs-agent-helm-os-deploy)
- [Monitoring a Red Hat OpenShift cluster](/docs/monitoring?topic=monitoring-openshift_cluster)


## Using the cluster logging operator
{: #oc_logging_operator}

To deploy the OpenShift Container Platform cluster logging operator and stack on your {{site.data.keyword.openshiftlong_notm}} cluster, see the [{{site.data.keyword.redhat_openshift_notm}} documentation](https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/logging/index){: external}. Additionally, you must update the cluster logging instance to use an {{site.data.keyword.cloud_notm}} Block Storage storage class.
{: shortdesc}

1. Prepare your worker pool to run the operator.
    1. Create a [VPC](/docs/openshift?topic=openshift-add-workers-vpc) or [classic](/docs/openshift?topic=openshift-add-workers-classic) worker pool with a flavor of **at least 4 cores and 32 GB memory** and 3 worker nodes.
    2. [Label the worker pool](/docs/openshift?topic=openshift-worker-tag-label).
    3. [Taint the worker pool](/docs/openshift?topic=openshift-kubernetes-service-cli#worker_pool_taint) so that other workloads can't run on the worker pool.
2. [Access your {{site.data.keyword.redhat_openshift_notm}} cluster](/docs/openshift?topic=openshift-access_cluster).
3. From the {{site.data.keyword.redhat_openshift_notm}} web console **Administrator** perspective, click **Operators > Installed Operators**.
4. Click **Cluster Logging**.
5. In the **Provided APIs** section, **Cluster Logging** tile, click **Create Instance**.
6. Modify the configuration YAML to change the storage class for the ElasticSearch log storage from `gp2` to one of the following storage classes that vary with your cluster infrastructure provider.
    * **Classic clusters**: `ibmc-block-gold`
    * **VPC clusters**: `ibmc-vpc-block-10iops-tier`

    ```yaml
    ...
        elasticsearch:
          nodeCount: 3
          redundancyPolicy: SingleRedundancy
          storage:
            storageClassName: ibmc-block-gold #or ibmc-vpc-block-10iops-tier for VPC clusters
            size: 200G
    ...
    ```
    {: codeblock}
    
7. Modify the configuration YAML to include the node selector and toleration for the worker pool label and taint that you previously created. For more information and examples, see the following {{site.data.keyword.redhat_openshift_notm}} documents. The examples use a label and toleration of `logging: clo-efk`.
    * [Node selector](https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/logging/index){: external}. Add the node selector to the Elasticsearch (`logstore`)and Kibana (`visualization`), and Fluentd (`collector.logs`) pods.
        ```yaml
        spec:
        logStore:
          elasticsearch:
            nodeSelector:
              logging: clo-efk
        ...
        visualization:
          kibana:
            nodeSelector:
              logging: clo-efk
        ...
        collection:
          logs:
            fluentd:
              nodeSelector:
                logging: clo-efk
        ```
        {: codeblock}

    * [Toleration](https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/nodes/controlling-pod-placement-onto-nodes-scheduling#nodes-scheduler-taints-tolerations-about_nodes-scheduler-taints-tolerations){: external}. Add the node selector to the Elasticsearch (`logstore`)and Kibana (`visualization`), and Fluentd (`collector.logs`) pods.
        ```yaml
        spec:
        logStore:
          elasticsearch:
            tolerations:
            - key: app
              value: clo-efk
              operator: "Exists"
              effect: "NoExecute"
        ...
        visualization:
          kibana:
            tolerations:
            - key: app
              value: clo-efk
              operator: "Exists"
              effect: "NoExecute"
        ...
        collection:
          logs:
            fluentd:
              tolerations:
              - key: app
                value: clo-efk
                operator: "Exists"
                effect: "NoExecute"
        ```
        {: codeblock}

8. Click **Create**.
9. Verify that the operator, Elasticsearch, Fluentd, and Kibana pods are all **Running**.
